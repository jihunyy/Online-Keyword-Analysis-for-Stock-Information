{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5d1e70a",
   "metadata": {},
   "source": [
    "### 기본 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a447610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.insert(0, os.path.abspath('../'))\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from transformers import (\n",
    "    AlbertConfig,\n",
    "    AlbertForSequenceClassification,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from tokenization_kbalbert import KbAlbertCharTokenizer\n",
    "from utils_stock import InputDataset, Split, get_label, convert_examples_to_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec48f58c",
   "metadata": {},
   "source": [
    "### 감성분석 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "647feaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용 데이터셋(15000개) 불러오기 \n",
    "train = pd.read_table('./input_data/ratings_train.txt')\n",
    "\n",
    "# 검증용 데이터셋(1000개) 불러오기\n",
    "test = pd.read_table('./input_data/ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01a05c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "\n",
    "        \n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    data_dir: str = field(\n",
    "        metadata={\"help\": \"The input data dir. Should contain the .txt files for dataset.\"}\n",
    "    )\n",
    "    labels: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Path to a file containing all labels. If not specified, labels are used.\"},\n",
    "    )\n",
    "    max_seq_len: int = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "        \n",
    "\n",
    "def main():\n",
    "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "    model_args, data_args, training_args = parser.parse_json_file(json_file='C:/Users/user/Desktop/ohae/OHAE_project/input_data/stock_config.json')\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        training_args.local_rank,\n",
    "        training_args.device,\n",
    "        training_args.n_gpu,\n",
    "        bool(training_args.local_rank != -1),\n",
    "        training_args.fp16,\n",
    "    )\n",
    "    logger.info(\"Training/evaluation parameters %s\", training_args)\n",
    "\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    labels = get_label()\n",
    "    print(labels)\n",
    "    label_map = {i: label for i, label in enumerate(labels)}\n",
    "    num_labels = len(labels)\n",
    "    print(num_labels)\n",
    "    config = AlbertConfig.from_pretrained(\n",
    "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "        num_labels=num_labels, id2label=label_map)\n",
    "    tokenizer = KbAlbertCharTokenizer.from_pretrained(\n",
    "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,)\n",
    "    model = AlbertForSequenceClassification.from_pretrained(\n",
    "        model_args.config_name if model_args.config_name else model_args.model_name_or_path, config=config)\n",
    "\n",
    "    train_dataset = (\n",
    "        InputDataset(\n",
    "            data_dir=data_args.data_dir,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_len=data_args.max_seq_len,\n",
    "            overwrite_cache=data_args.overwrite_cache,\n",
    "            mode=Split.train\n",
    "        )\n",
    "        if training_args.do_train\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    eval_dataset = (\n",
    "        InputDataset(\n",
    "            data_dir=data_args.data_dir,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_len=data_args.max_seq_len,\n",
    "            overwrite_cache=data_args.overwrite_cache,\n",
    "            mode=Split.test\n",
    "        )\n",
    "        if training_args.do_eval\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    def align_predictions(predictions: np.ndarray, label_ids: np.ndarray) -> Tuple[List[int], List[int]]:\n",
    "        preds = np.argmax(predictions, axis=1)\n",
    "\n",
    "        return preds, label_ids\n",
    "\n",
    "    def compute_metrics(p: EvalPrediction) -> Dict:\n",
    "        preds_list, out_label_list = align_predictions(p.predictions, p.label_ids)\n",
    "        return {\n",
    "            'acc': (preds_list == out_label_list).mean()\n",
    "        }\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    if training_args.do_train:\n",
    "        trainer.train(\n",
    "            model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n",
    "        )\n",
    "        trainer.save_model(training_args.output_dir)\n",
    "        tokenizer.save_pretrained(training_args.output_dir)\n",
    "\n",
    "    results = {}\n",
    "    if training_args.do_eval:\n",
    "        logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "        result = trainer.evaluate()\n",
    "\n",
    "        output_eval_file = os.path.join(training_args.output_dir, \"eval_results.txt\")\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"***** Eval results *****\")\n",
    "            for key, value in result.items():\n",
    "                logger.info(\"  %s = %s\", key, value)\n",
    "                writer.write(\"%s = %s\\n\" % (key, value))\n",
    "\n",
    "        results.update(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def _mp_fn(index):\n",
    "    main()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
